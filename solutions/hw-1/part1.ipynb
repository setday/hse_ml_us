{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1\n",
    "\n",
    "Процедура генерация датасета:\n",
    "Сгенерируйте датасет с 10000 наблюдений и 1000 колонок (сэмплируйте из разных распеределений) и сформируйте из него таргет на сонове 100 колонок + зашумление (общее или небольшое для каждой колонки - постарайтесь сделать так чтобы шум не сильно влиял на корреляции между предикторами и таргетам). Удостоверьтесь, что в датасете существуют колонки, которые не использовались для таргета, но при этом имеют высокую корреляцию с теми, что использовались (покажите это в коде).\n",
    "\n",
    "Реализуйте forward stage wise регрессию стандартным образом и с помощью QR разложения наиболее быстрым образом (засекайте время для всех опробованных вариантов). Замерьте качество и процент колонок, которые были правильно найдены.\n",
    "\n",
    "**Дополнительно**: \n",
    "Попробуйте генерировать данные таким образом, чтобы ошибка постепенно ухудшалась. Подсказка: увеличивайте шум, используйте нелинейные функции и комбинации предикторов. Попробуйте оценить bias и variance для forward stage-wise regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTOR_CNT = 1_000\n",
    "TARGET_PREDICTOR_CNT = 100\n",
    "SAMPLE_CNT = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributionGenerator:\n",
    "    @staticmethod\n",
    "    def generate_random_exponential(n: int = 10000, scale: float | None = None):\n",
    "        \"\"\"\n",
    "        Generate random exponential distribution\n",
    "        :param n: number of samples\n",
    "        :param scale: scale parameter of the exponential distribution\n",
    "        :return: np.array of random samples\n",
    "        \"\"\"\n",
    "\n",
    "        if scale is None:\n",
    "            scale = np.random.uniform(0.75, 2.0)\n",
    "        return np.random.exponential(scale=scale, size=n)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_random_uniform(n: int = 10000, low: float | None = None, length: float | None = None):\n",
    "        \"\"\"\n",
    "        Generate random uniform distribution\n",
    "        :param n: number of samples\n",
    "        :param low: lower bound of the uniform distribution\n",
    "        :param length: length of the uniform distribution\n",
    "        :return: np.array of random samples\n",
    "        \"\"\"\n",
    "\n",
    "        # Kekw: using uniform to generate limits to uniform\n",
    "        if low is None:\n",
    "            low = np.random.uniform(-2.0, -1.0)\n",
    "        if length is None:\n",
    "            length = np.random.uniform(1.0, 3.0)\n",
    "        return np.random.uniform(low=low, high=low + length, size=n)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_random_normal(n: int = 10000, loc: float | None = None, scale: float | None = None):\n",
    "        \"\"\"\n",
    "        Generate random normal distribution\n",
    "        :param n: number of samples\n",
    "        :param loc: mean of the normal distribution\n",
    "        :param scale: standard deviation of the normal distribution\n",
    "        :return: np.array of random samples\n",
    "        \"\"\"\n",
    "\n",
    "        if loc is None:\n",
    "            loc = np.random.uniform(-1.0, 1.0)\n",
    "        if scale is None:\n",
    "            scale = np.random.uniform(0.2, 2.0)\n",
    "        return np.random.normal(loc=loc, scale=scale, size=n)\n",
    "\n",
    "    distribution_fn_list = [\n",
    "        # generate_random_exponential,\n",
    "        # generate_random_uniform,\n",
    "        generate_random_normal,\n",
    "    ]\n",
    "    @staticmethod\n",
    "    def generate_random_distribution(n: int = 10000):\n",
    "        \"\"\"\n",
    "        Generate random distribution\n",
    "        :param n: number of samples\n",
    "        :return: np.array of random samples\n",
    "        \"\"\"\n",
    "\n",
    "        # Choose a random distribution function\n",
    "        distribution_fn = np.random.choice(DistributionGenerator.distribution_fn_list)\n",
    "        return distribution_fn(n)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_random_noice(n: int = 10000, limit: float = 0.05):\n",
    "        \"\"\"\n",
    "        Generate random noice based on uniform distribution\n",
    "        :param n: number of samples\n",
    "        :param limit: limit of the uniform distribution\n",
    "        :return: np.array of random noice\n",
    "        \"\"\"\n",
    "        return DistributionGenerator.generate_random_uniform(n, -limit, limit * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictors_targets_table(noise_level: float = 0.05):\n",
    "    \"\"\"\n",
    "    Create a table with predictors and target\n",
    "    :param noise_level: noise level of the target\n",
    "    :return: pandas data frame with predictors and target, selected predictors and their coefficients (in tuple form)\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate random data\n",
    "    data = {\n",
    "        f\"predictor_{i}\": DistributionGenerator.generate_random_distribution(SAMPLE_CNT)\n",
    "        for i in range(PREDICTOR_CNT)\n",
    "    }\n",
    "\n",
    "    # Choose random predictors for the target and generate coefficients for them\n",
    "    all_predictors = list(data.keys())\n",
    "    selected_predictors = np.random.choice(a=all_predictors, size=TARGET_PREDICTOR_CNT, replace=False)\n",
    "    predictors_koefs = DistributionGenerator.generate_random_uniform(TARGET_PREDICTOR_CNT, 1.0, 2.0)\n",
    "\n",
    "    # Generate target based on the selected predictors\n",
    "    target_predictors_matrix = np.array([\n",
    "        data[predictor] for predictor in selected_predictors\n",
    "    ])\n",
    "    target = predictors_koefs @ target_predictors_matrix\n",
    "\n",
    "    # Add noice to the target\n",
    "    noice = DistributionGenerator.generate_random_noice(SAMPLE_CNT, limit=noise_level)\n",
    "    data[\"target\"] = target + noice\n",
    "\n",
    "    # Create a pandas data frame\n",
    "    data_frame = pd.DataFrame(data)\n",
    "    return data_frame, selected_predictors, predictors_koefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictor_0</th>\n",
       "      <th>predictor_1</th>\n",
       "      <th>predictor_2</th>\n",
       "      <th>predictor_3</th>\n",
       "      <th>predictor_4</th>\n",
       "      <th>predictor_5</th>\n",
       "      <th>predictor_6</th>\n",
       "      <th>predictor_7</th>\n",
       "      <th>predictor_8</th>\n",
       "      <th>predictor_9</th>\n",
       "      <th>...</th>\n",
       "      <th>predictor_991</th>\n",
       "      <th>predictor_992</th>\n",
       "      <th>predictor_993</th>\n",
       "      <th>predictor_994</th>\n",
       "      <th>predictor_995</th>\n",
       "      <th>predictor_996</th>\n",
       "      <th>predictor_997</th>\n",
       "      <th>predictor_998</th>\n",
       "      <th>predictor_999</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.985958</td>\n",
       "      <td>0.065594</td>\n",
       "      <td>-0.846876</td>\n",
       "      <td>-4.593663</td>\n",
       "      <td>2.487855</td>\n",
       "      <td>-0.971370</td>\n",
       "      <td>0.780931</td>\n",
       "      <td>-0.255971</td>\n",
       "      <td>0.321463</td>\n",
       "      <td>-0.153328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694411</td>\n",
       "      <td>0.622125</td>\n",
       "      <td>0.918961</td>\n",
       "      <td>-4.613140</td>\n",
       "      <td>-2.948084</td>\n",
       "      <td>0.301764</td>\n",
       "      <td>-0.499084</td>\n",
       "      <td>-1.166346</td>\n",
       "      <td>1.813773</td>\n",
       "      <td>21.689586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.935263</td>\n",
       "      <td>2.123331</td>\n",
       "      <td>0.480637</td>\n",
       "      <td>-0.765075</td>\n",
       "      <td>-2.103570</td>\n",
       "      <td>-1.057442</td>\n",
       "      <td>-1.031373</td>\n",
       "      <td>-0.776426</td>\n",
       "      <td>-0.464095</td>\n",
       "      <td>-1.199590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189023</td>\n",
       "      <td>1.288292</td>\n",
       "      <td>0.574721</td>\n",
       "      <td>-1.042175</td>\n",
       "      <td>-0.981104</td>\n",
       "      <td>-0.646160</td>\n",
       "      <td>-4.227038</td>\n",
       "      <td>-0.645952</td>\n",
       "      <td>1.374093</td>\n",
       "      <td>-19.015062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.105202</td>\n",
       "      <td>-0.161361</td>\n",
       "      <td>0.391861</td>\n",
       "      <td>-0.346945</td>\n",
       "      <td>-0.466181</td>\n",
       "      <td>-1.403384</td>\n",
       "      <td>-0.143361</td>\n",
       "      <td>-0.092815</td>\n",
       "      <td>1.007678</td>\n",
       "      <td>-0.644908</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.837571</td>\n",
       "      <td>0.937798</td>\n",
       "      <td>-0.035396</td>\n",
       "      <td>-0.089725</td>\n",
       "      <td>1.002345</td>\n",
       "      <td>-0.488152</td>\n",
       "      <td>-0.609783</td>\n",
       "      <td>-0.615746</td>\n",
       "      <td>0.218893</td>\n",
       "      <td>-4.965162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.467000</td>\n",
       "      <td>-1.882663</td>\n",
       "      <td>-1.057082</td>\n",
       "      <td>1.000836</td>\n",
       "      <td>-0.863687</td>\n",
       "      <td>-1.345843</td>\n",
       "      <td>0.918456</td>\n",
       "      <td>0.085471</td>\n",
       "      <td>0.238856</td>\n",
       "      <td>-0.398822</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120372</td>\n",
       "      <td>-0.470308</td>\n",
       "      <td>0.769316</td>\n",
       "      <td>-2.296704</td>\n",
       "      <td>1.093235</td>\n",
       "      <td>-0.492854</td>\n",
       "      <td>-0.467187</td>\n",
       "      <td>-0.094619</td>\n",
       "      <td>-1.330087</td>\n",
       "      <td>-22.660378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.126857</td>\n",
       "      <td>1.720015</td>\n",
       "      <td>-2.169759</td>\n",
       "      <td>-2.673064</td>\n",
       "      <td>0.339270</td>\n",
       "      <td>-0.948636</td>\n",
       "      <td>1.141215</td>\n",
       "      <td>-0.376301</td>\n",
       "      <td>3.175915</td>\n",
       "      <td>0.206297</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.275861</td>\n",
       "      <td>0.119540</td>\n",
       "      <td>-0.420503</td>\n",
       "      <td>3.237286</td>\n",
       "      <td>1.065692</td>\n",
       "      <td>-0.788488</td>\n",
       "      <td>-1.717213</td>\n",
       "      <td>-0.959444</td>\n",
       "      <td>-2.929430</td>\n",
       "      <td>-18.193260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   predictor_0  predictor_1  predictor_2  predictor_3  predictor_4  \\\n",
       "0    -1.985958     0.065594    -0.846876    -4.593663     2.487855   \n",
       "1     0.935263     2.123331     0.480637    -0.765075    -2.103570   \n",
       "2     1.105202    -0.161361     0.391861    -0.346945    -0.466181   \n",
       "3     3.467000    -1.882663    -1.057082     1.000836    -0.863687   \n",
       "4     2.126857     1.720015    -2.169759    -2.673064     0.339270   \n",
       "\n",
       "   predictor_5  predictor_6  predictor_7  predictor_8  predictor_9  ...  \\\n",
       "0    -0.971370     0.780931    -0.255971     0.321463    -0.153328  ...   \n",
       "1    -1.057442    -1.031373    -0.776426    -0.464095    -1.199590  ...   \n",
       "2    -1.403384    -0.143361    -0.092815     1.007678    -0.644908  ...   \n",
       "3    -1.345843     0.918456     0.085471     0.238856    -0.398822  ...   \n",
       "4    -0.948636     1.141215    -0.376301     3.175915     0.206297  ...   \n",
       "\n",
       "   predictor_991  predictor_992  predictor_993  predictor_994  predictor_995  \\\n",
       "0       0.694411       0.622125       0.918961      -4.613140      -2.948084   \n",
       "1       0.189023       1.288292       0.574721      -1.042175      -0.981104   \n",
       "2      -0.837571       0.937798      -0.035396      -0.089725       1.002345   \n",
       "3      -0.120372      -0.470308       0.769316      -2.296704       1.093235   \n",
       "4      -0.275861       0.119540      -0.420503       3.237286       1.065692   \n",
       "\n",
       "   predictor_996  predictor_997  predictor_998  predictor_999     target  \n",
       "0       0.301764      -0.499084      -1.166346       1.813773  21.689586  \n",
       "1      -0.646160      -4.227038      -0.645952       1.374093 -19.015062  \n",
       "2      -0.488152      -0.609783      -0.615746       0.218893  -4.965162  \n",
       "3      -0.492854      -0.467187      -0.094619      -1.330087 -22.660378  \n",
       "4      -0.788488      -1.717213      -0.959444      -2.929430 -18.193260  \n",
       "\n",
       "[5 rows x 1001 columns]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame, selected_predictors_true, predictors_koefs_true = create_predictors_targets_table()\n",
    "\n",
    "# Separate predictors and target\n",
    "X = data_frame.loc[:, data_frame.columns != 'target'].to_numpy()\n",
    "y = data_frame[\"target\"].to_numpy()\n",
    "\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum correlation of selected predictors - target pairs is -0.006135753649864581\n",
      "Maximum correlation of non-selected predictors - target pairs is 0.03129463161381169\n",
      "At least five pairs of non-selected predictors - selected predictors have correlation not less than 0.03687704123369397\n"
     ]
    }
   ],
   "source": [
    "def show_predictors_targets_table_correlation_info(data_frame: pd.DataFrame, selected_predictors_true: np.ndarray):\n",
    "    \"\"\"\n",
    "    Show correlation information between predictors and target\n",
    "    :param data_frame: pandas data frame with predictors and target\n",
    "    :param selected_predictors_true: true selected predictors\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate correlation table for all the predictors and target\n",
    "    correlation_table = data_frame.corr()\n",
    "\n",
    "    # Calculate correlation information for the selected predictors for the target\n",
    "    selected_correlations = correlation_table[\"target\"][selected_predictors_true].to_numpy()\n",
    "    min_selected_correlations = selected_correlations.min()\n",
    "\n",
    "    # Calculate correlation information for the non-selected predictors for the target\n",
    "    non_selected_correlations = correlation_table[\"target\"].drop(labels=selected_predictors_true).drop(labels=\"target\").to_numpy()\n",
    "    max_non_selected_correlations = non_selected_correlations.max()\n",
    "\n",
    "    # Calculate correlation information for the non-selected predictors for the selected predictors\n",
    "    predictors_correlation_table = correlation_table.drop(columns=\"target\").drop(labels=\"target\")\n",
    "    numpy_correlation_table = predictors_correlation_table[selected_predictors_true].drop(labels=selected_predictors_true).to_numpy()\n",
    "    np.fill_diagonal(numpy_correlation_table, 0.0)\n",
    "    five_most_correlated = sorted(numpy_correlation_table.flatten(), reverse=True)[:5 * 2]\n",
    "\n",
    "    print(\"Minimum correlation of selected predictors - target pairs is\", min_selected_correlations)\n",
    "    print(\"Maximum correlation of non-selected predictors - target pairs is\", max_non_selected_correlations)\n",
    "    print(\"At least five pairs of non-selected predictors - selected predictors have correlation not less than\", five_most_correlated[-1])\n",
    "\n",
    "show_predictors_targets_table_correlation_info(data_frame, selected_predictors_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QR decomposition is correct\n"
     ]
    }
   ],
   "source": [
    "def QR_decomposition(X: np.ndarray): # ~3.4s on test (1001, 1003)\n",
    "    \"\"\"\n",
    "    Perform QR decomposition of the matrix X (self-written)\n",
    "    :param X: matrix to decompose\n",
    "    :return: Q and R matrices\n",
    "    \"\"\"\n",
    "\n",
    "    Q = np.zeros_like(X)\n",
    "    R = np.zeros((X.shape[1], X.shape[1]))\n",
    "\n",
    "    for i in range(X.shape[1]):\n",
    "        Q[:, i] = X[:, i]\n",
    "        for j in range(i):\n",
    "            R[j, i] = Q[:, j] @ Q[:, i]\n",
    "            Q[:, i] -= R[j, i] * Q[:, j]\n",
    "        R[i, i] = np.linalg.norm(Q[:, i])\n",
    "        Q[:, i] /= R[i, i]\n",
    "\n",
    "    return Q, R\n",
    "\n",
    "def QR_decomposition_fast(X: np.ndarray): # ~0.2s on test (1001, 1003)\n",
    "    \"\"\"\n",
    "    Perform QR decomposition of the matrix X (numpy)\n",
    "    :param X: matrix to decompose\n",
    "    :return: Q and R matrices\n",
    "    \"\"\"\n",
    "\n",
    "    return np.linalg.qr(X)\n",
    "\n",
    "def test_QR_decomposition():\n",
    "    \"\"\"\n",
    "    Test QR decomposition\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.random.rand(1001, 1003)\n",
    "    Q, R = QR_decomposition(A)\n",
    "    D = Q @ R - A\n",
    "    if max(-D.min(), D.max()) < 1e-6:\n",
    "        print(\"QR decomposition is correct\")\n",
    "    else:\n",
    "        print(\"QR decomposition is incorrect\")\n",
    "\n",
    "test_QR_decomposition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncrementalForwardStagewiseRegression:\n",
    "    def __init__(self, max_iter: int = 1_500_000, tol: float = 1e-3, step: float = 1e-2, step_decay: float = 0.8, silent: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the class\n",
    "        :param max_iter: maximum number of iterations\n",
    "        :param tol: tolerance for the correlation\n",
    "        :param step: step size\n",
    "        :param step_decay: step decay\n",
    "        :param silent\n",
    "        Default values are specially selected for fit v3\n",
    "        For fit v1 and v2 use max_iter: int = 15_000, tol: float = 1e-3, step: float = 1e-3\n",
    "        \"\"\"\n",
    "\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.step = step\n",
    "        self.step_decay = step_decay\n",
    "\n",
    "        self.silent = silent\n",
    "\n",
    "        self.residual = None\n",
    "        self.beta = None\n",
    "\n",
    "        self.X_mean = None\n",
    "        self.X_std = None\n",
    "\n",
    "        self.y_mean = None\n",
    "        self.y_std = None\n",
    "\n",
    "    def _fit_normalization(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Normalize data\n",
    "        :param X: matrix of predictors\n",
    "        :param y: target vector\n",
    "        \"\"\"\n",
    "\n",
    "        self.X_mean = X.mean(axis=0)\n",
    "        self.X_std = X.std(axis=0)\n",
    "        self.y_mean = y.mean()\n",
    "        self.y_std = y.std()\n",
    "\n",
    "        X = (X - self.X_mean) / self.X_std\n",
    "        y = (y - self.y_mean) / self.y_std\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def _normileze_X(self, X: np.ndarray):\n",
    "        return (X - self.X_mean) / self.X_std\n",
    "    \n",
    "    def _unnormalize_y(self, y: np.ndarray):\n",
    "        return y * self.y_std + self.y_mean\n",
    "\n",
    "\n",
    "    def _fit_v1(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Perform incremental forward stagewise regression\n",
    "        :param X: matrix of predictors\n",
    "        :param y: target vector\n",
    "        \"\"\"\n",
    "\n",
    "        # Normalize data\n",
    "        X, y = self._fit_normalization(X, y)\n",
    "\n",
    "        # Initialize variables\n",
    "        residual = y.copy()\n",
    "        self.beta = np.zeros(X.shape[1])\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            # Calculate correlations (slow)\n",
    "            correlations = X.T @ residual\n",
    "            correlations_magnitude = np.abs(correlations)\n",
    "\n",
    "            # Find the best predictor (the one with the highest correlation)\n",
    "            best_predictor = np.argmax(correlations_magnitude)\n",
    "            best_correlation = correlations_magnitude[best_predictor]\n",
    "\n",
    "            # Check if the best correlation is less than the tolerance. So the beta is predicting the target well enough\n",
    "            if best_correlation < self.tol:\n",
    "                print(\"Converged at iteration\", _)\n",
    "                break\n",
    "\n",
    "            # Update beta and residual\n",
    "            self.beta[best_predictor] += self.step * np.sign(correlations[best_predictor])\n",
    "            residual -= self.step * np.sign(correlations[best_predictor]) * X[:, best_predictor]\n",
    "            \n",
    "            if not self.silent and _ != 0 and _ % 1_000 == 0:\n",
    "                train_loss = (residual ** 2).mean()\n",
    "                print(f\"Iteration {_}, train loss: {train_loss} (MSE)\")\n",
    "        else:\n",
    "            print(\"Warning: maximum number of iterations reached. Limit was\", self.max_iter)\n",
    "\n",
    "    def _fit_v2(self, X: np.ndarray, y: np.ndarray):\n",
    "        X, y = self._fit_normalization(X, y)\n",
    "\n",
    "        residual = y.copy()\n",
    "        self.beta = np.zeros(X.shape[1])\n",
    "\n",
    "        correlations = X.T @ residual\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            correlations_magnitude = np.abs(correlations)\n",
    "\n",
    "            best_predictor = np.argmax(correlations_magnitude)\n",
    "            best_correlation = correlations_magnitude[best_predictor]\n",
    "\n",
    "            if best_correlation < self.tol:\n",
    "                print(\"Converged at iteration\", _)\n",
    "                break\n",
    "\n",
    "            # Update beta and correlations\n",
    "            self.beta[best_predictor] += self.step * np.sign(correlations[best_predictor])\n",
    "            correlations -= self.step * np.sign(correlations[best_predictor]) * (X.T @ X[:, best_predictor])\n",
    "\n",
    "            if not self.silent:\n",
    "                residual -= self.step * np.sign(correlations[best_predictor]) * X[:, best_predictor]\n",
    "            \n",
    "                if _ != 0 and _ % 1_000 == 0:\n",
    "                    train_loss = (residual ** 2).mean()\n",
    "                    print(f\"Iteration {_}, train loss: {train_loss} (MSE)\")\n",
    "        else:\n",
    "            print(\"Warning: maximum number of iterations reached. Limit was\", self.max_iter)\n",
    "\n",
    "    def _fit_v3(\n",
    "            self,\n",
    "            X: np.ndarray,\n",
    "            y: np.ndarray,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Perform incremental forward stagewise regression\n",
    "        (the fastest method without multiplication of the whole matrix)\n",
    "        :param X: matrix of predictors\n",
    "        :param y: target vector\n",
    "        :param max_iter: maximum number of iterations\n",
    "        :param tol: tolerance for the correlation\n",
    "        :param step: step size\n",
    "        :param step_decay: step decay\n",
    "        \"\"\"\n",
    "\n",
    "        X, y = self._fit_normalization(X, y)\n",
    "\n",
    "        step = self.step\n",
    "\n",
    "        residual = y.copy()\n",
    "        self.beta = np.zeros(X.shape[1])\n",
    "\n",
    "        correlations = X.T @ residual\n",
    "\n",
    "        XTX = X.T @ X\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            correlations_magnitude = np.abs(correlations)\n",
    "\n",
    "            best_predictor = np.argmax(correlations_magnitude)\n",
    "            best_correlation = correlations_magnitude[best_predictor]\n",
    "\n",
    "            if best_correlation < self.tol:\n",
    "                print(\"Converged at iteration\", _)\n",
    "                break\n",
    "\n",
    "            # Update beta and correlations\n",
    "            self.beta[best_predictor] += step * np.sign(correlations[best_predictor])\n",
    "            correlations -= step * np.sign(correlations[best_predictor]) * XTX[:, best_predictor]\n",
    "\n",
    "            if not self.silent:\n",
    "                residual -= step * np.sign(correlations[best_predictor]) * X[:, best_predictor]\n",
    "\n",
    "                if _ != 0 and _ % (self.max_iter // 100) == 0:\n",
    "                    train_loss = (residual ** 2).mean()\n",
    "                    print(f\"Iteration {_}, train loss: {train_loss} (MSE)\")\n",
    "\n",
    "            if _ != 0 and _ % 10_000 == 0:\n",
    "                step *= self.step_decay\n",
    "        else:\n",
    "            print(\"Warning: maximum number of iterations reached. Limit was\", self.max_iter)\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Fit the model\n",
    "        :param X: matrix of predictors\n",
    "        :param y: target vector\n",
    "        \"\"\"\n",
    "\n",
    "        # self._fit_v1(X, y) # ~19.7s on 15_000 iterations\n",
    "        # self._fit_v2(X, y) # ~19.3s on 15_000 iterations\n",
    "        self._fit_v3(X, y) # ~3.4s on 500_000 iterations (No matrix multiplication in the loop)\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Predict the target\n",
    "        :param X: matrix of predictors\n",
    "        :return: predicted target\n",
    "        \"\"\"\n",
    "\n",
    "        if self.beta is None:\n",
    "            raise ValueError(\"Model is not fitted\")\n",
    "\n",
    "        return self._unnormalize_y(self._normileze_X(X) @ self.beta)\n",
    "    \n",
    "    def compare_coefficients_with_true(self, selected_predictors_true: np.ndarray):\n",
    "        \"\"\"\n",
    "        Compare coefficients with the true ones\n",
    "        :param selected_predictors_true: true selected predictors\n",
    "        \"\"\"\n",
    "\n",
    "        # Find the coefficients that are not zero\n",
    "        coeffs_found = (self.beta > self.tol).sum()\n",
    "        print(f\"Found {coeffs_found} coefficients out of {len(selected_predictors_true)}\")\n",
    "\n",
    "        matched_coeffs = 0\n",
    "\n",
    "        for predictor in selected_predictors_true:\n",
    "            predictor_id = int(predictor.split(\"_\")[-1])\n",
    "            \n",
    "            # There should be a coefficient comparator (self.beta vs predictors_koefs_true),\n",
    "            # but beta was trained on normalized data and predictors_koefs_true used on non-normalized data\n",
    "            # So, we can't compare them =(\n",
    "            if self.beta[predictor_id] > self.tol:\n",
    "                matched_coeffs += 1\n",
    "\n",
    "        print(f\"Matched {matched_coeffs} coefficients out of {len(selected_predictors_true)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15000, train loss: 0.013777021450034083 (MSE)\n",
      "Iteration 30000, train loss: 0.013209539052058178 (MSE)\n",
      "Iteration 45000, train loss: 0.012706747128656406 (MSE)\n",
      "Iteration 60000, train loss: 0.012384810330501553 (MSE)\n",
      "Iteration 75000, train loss: 0.01217562099452295 (MSE)\n",
      "Iteration 90000, train loss: 0.012147906762249423 (MSE)\n",
      "Iteration 105000, train loss: 0.011903439340542712 (MSE)\n",
      "Iteration 120000, train loss: 0.012074409192485154 (MSE)\n",
      "Iteration 135000, train loss: 0.011968114377936868 (MSE)\n",
      "Iteration 150000, train loss: 0.012127480490518656 (MSE)\n",
      "Iteration 165000, train loss: 0.012023185917441557 (MSE)\n",
      "Iteration 180000, train loss: 0.012022784790884677 (MSE)\n",
      "Iteration 195000, train loss: 0.01203851588205002 (MSE)\n",
      "Iteration 210000, train loss: 0.012000156412136124 (MSE)\n",
      "Iteration 225000, train loss: 0.012025201415539541 (MSE)\n",
      "Iteration 240000, train loss: 0.01201633946956323 (MSE)\n",
      "Iteration 255000, train loss: 0.012021059386483304 (MSE)\n",
      "Iteration 270000, train loss: 0.012011379427699163 (MSE)\n",
      "Iteration 285000, train loss: 0.01201746860784431 (MSE)\n",
      "Iteration 300000, train loss: 0.01201963068331183 (MSE)\n",
      "Iteration 315000, train loss: 0.012018656442315008 (MSE)\n",
      "Iteration 330000, train loss: 0.01201761519751275 (MSE)\n",
      "Iteration 345000, train loss: 0.012017587512077451 (MSE)\n",
      "Iteration 360000, train loss: 0.012017575778506004 (MSE)\n",
      "Iteration 375000, train loss: 0.012017790353373302 (MSE)\n",
      "Iteration 390000, train loss: 0.012017878494104116 (MSE)\n",
      "Iteration 405000, train loss: 0.012018022432405673 (MSE)\n",
      "Iteration 420000, train loss: 0.0120180952796753 (MSE)\n",
      "Iteration 435000, train loss: 0.01201803426417771 (MSE)\n",
      "Iteration 450000, train loss: 0.01201804247470448 (MSE)\n",
      "Iteration 465000, train loss: 0.01201803238448984 (MSE)\n",
      "Iteration 480000, train loss: 0.012018058055909834 (MSE)\n",
      "Converged at iteration 490212\n",
      "Found 100 coefficients out of 100\n",
      "Matched 100 coefficients out of 100\n"
     ]
    }
   ],
   "source": [
    "model = IncrementalForwardStagewiseRegression(silent=False)\n",
    "model.fit(X, y)\n",
    "model.compare_coefficients_with_true(selected_predictors_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: maximum number of iterations reached. Limit was 50000\n",
      "MSE: 0.5299552900259948 | MAE: 0.52743384496775\n"
     ]
    }
   ],
   "source": [
    "def test_on_real_data(model):\n",
    "    \"\"\"\n",
    "    Test the model on real data\n",
    "    :param model: model to test\n",
    "    \"\"\"\n",
    "\n",
    "    data_california = fetch_california_housing()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_california.data, data_california.target, test_size=0.3, random_state=42)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"MSE: {mse} | MAE: {mae}\")\n",
    "\n",
    "test_on_real_data(IncrementalForwardStagewiseRegression(max_iter=50_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.5305677824766757 | MAE: 0.5272474538306168\n"
     ]
    }
   ],
   "source": [
    "test_on_real_data(LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_non_linear_predictors_targets_table(noise_level: float = 0.5):\n",
    "    \"\"\"\n",
    "    Create a table with predictors and target with non-linear dependencies\n",
    "    :param noise_level: noise level of the target\n",
    "    :return: pandas data frame with predictors and target, selected predictors and their coefficients (in tuple form)\n",
    "    \"\"\"\n",
    "    \n",
    "    data = {\n",
    "        f\"predictor_{i}\": DistributionGenerator.generate_random_uniform(SAMPLE_CNT, 0, 1)\n",
    "        for i in range(PREDICTOR_CNT)\n",
    "    }\n",
    "\n",
    "    target_predictors_matrix = np.array([\n",
    "        data[f\"predictor_{predictor}\"] for predictor in range(TARGET_PREDICTOR_CNT)\n",
    "    ])\n",
    "    target = np.log(target_predictors_matrix).sum(axis=0)\n",
    "\n",
    "    # Add noice to the target\n",
    "    noice = DistributionGenerator.generate_random_noice(SAMPLE_CNT, limit=noise_level)\n",
    "    data[\"target\"] = target + noice\n",
    "\n",
    "    # Create a pandas data frame\n",
    "    data_frame = pd.DataFrame(data)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, train loss: 0.9739599645160469 (MSE)\n",
      "Iteration 24, train loss: 0.9524603439382854 (MSE)\n",
      "Iteration 36, train loss: 0.9316979494398167 (MSE)\n",
      "Iteration 48, train loss: 0.9114464109443714 (MSE)\n",
      "Iteration 60, train loss: 0.8916318152396824 (MSE)\n",
      "Iteration 72, train loss: 0.8722918921122684 (MSE)\n",
      "Iteration 84, train loss: 0.8535305715745337 (MSE)\n",
      "Iteration 96, train loss: 0.8351337779474821 (MSE)\n",
      "Iteration 108, train loss: 0.8170125073070176 (MSE)\n",
      "Iteration 120, train loss: 0.7992305480991873 (MSE)\n",
      "Iteration 132, train loss: 0.7818274933568343 (MSE)\n",
      "Iteration 144, train loss: 0.7646602738566871 (MSE)\n",
      "Iteration 156, train loss: 0.7477287296011589 (MSE)\n",
      "Iteration 168, train loss: 0.7311186075760736 (MSE)\n",
      "Iteration 180, train loss: 0.7148927628440739 (MSE)\n",
      "Iteration 192, train loss: 0.6989710015036069 (MSE)\n",
      "Iteration 204, train loss: 0.6833277608701275 (MSE)\n",
      "Iteration 216, train loss: 0.6679204151365903 (MSE)\n",
      "Iteration 228, train loss: 0.6527582522061324 (MSE)\n",
      "Iteration 240, train loss: 0.6378841254538209 (MSE)\n",
      "Iteration 252, train loss: 0.6232009172502799 (MSE)\n",
      "Iteration 264, train loss: 0.6088146432206364 (MSE)\n",
      "Iteration 276, train loss: 0.5947358791967723 (MSE)\n",
      "Iteration 288, train loss: 0.5810719346106368 (MSE)\n",
      "Iteration 300, train loss: 0.5677016763664916 (MSE)\n",
      "Iteration 312, train loss: 0.5545974503499576 (MSE)\n",
      "Iteration 324, train loss: 0.5417668555992551 (MSE)\n",
      "Iteration 336, train loss: 0.5291250699475694 (MSE)\n",
      "Iteration 348, train loss: 0.5167092256597065 (MSE)\n",
      "Iteration 360, train loss: 0.5045115584059758 (MSE)\n",
      "Iteration 372, train loss: 0.49265355097508384 (MSE)\n",
      "Iteration 384, train loss: 0.48117422108014163 (MSE)\n",
      "Iteration 396, train loss: 0.47005579626511523 (MSE)\n",
      "Iteration 408, train loss: 0.45927579267990676 (MSE)\n",
      "Iteration 420, train loss: 0.4487147673363083 (MSE)\n",
      "Iteration 432, train loss: 0.4383560316317228 (MSE)\n",
      "Iteration 444, train loss: 0.42820803280114944 (MSE)\n",
      "Iteration 456, train loss: 0.41824080594179686 (MSE)\n",
      "Iteration 468, train loss: 0.4086393733389654 (MSE)\n",
      "Iteration 480, train loss: 0.39930737142366995 (MSE)\n",
      "Iteration 492, train loss: 0.3904580359791136 (MSE)\n",
      "Iteration 504, train loss: 0.38188701297768246 (MSE)\n",
      "Iteration 516, train loss: 0.37362772988076387 (MSE)\n",
      "Iteration 528, train loss: 0.36558155788872704 (MSE)\n",
      "Iteration 540, train loss: 0.35767986945721203 (MSE)\n",
      "Iteration 552, train loss: 0.3499924834631795 (MSE)\n",
      "Iteration 564, train loss: 0.3425669219918671 (MSE)\n",
      "Iteration 576, train loss: 0.33554625898328233 (MSE)\n",
      "Iteration 588, train loss: 0.3289041465002115 (MSE)\n",
      "Iteration 600, train loss: 0.32250667205695677 (MSE)\n",
      "Iteration 612, train loss: 0.3165059770303612 (MSE)\n",
      "Iteration 624, train loss: 0.3107402373522941 (MSE)\n",
      "Iteration 636, train loss: 0.3050910039330958 (MSE)\n",
      "Iteration 648, train loss: 0.29965470533282335 (MSE)\n",
      "Iteration 660, train loss: 0.2945077882212168 (MSE)\n",
      "Iteration 672, train loss: 0.2897884510854375 (MSE)\n",
      "Iteration 684, train loss: 0.2853641840385302 (MSE)\n",
      "Iteration 696, train loss: 0.28121560147331276 (MSE)\n",
      "Iteration 708, train loss: 0.2774146932564771 (MSE)\n",
      "Iteration 720, train loss: 0.2738521518070824 (MSE)\n",
      "Iteration 732, train loss: 0.27044354842338925 (MSE)\n",
      "Iteration 744, train loss: 0.2672437094643336 (MSE)\n",
      "Iteration 756, train loss: 0.26433847436101976 (MSE)\n",
      "Iteration 768, train loss: 0.2617853545766572 (MSE)\n",
      "Iteration 780, train loss: 0.25952504590814646 (MSE)\n",
      "Iteration 792, train loss: 0.2574530355275081 (MSE)\n",
      "Iteration 804, train loss: 0.2556033428567386 (MSE)\n",
      "Iteration 816, train loss: 0.25390504446879375 (MSE)\n",
      "Iteration 828, train loss: 0.25233154972706723 (MSE)\n",
      "Iteration 840, train loss: 0.25082701092067183 (MSE)\n",
      "Iteration 852, train loss: 0.24944118699869464 (MSE)\n",
      "Iteration 864, train loss: 0.24815042102622775 (MSE)\n",
      "Iteration 876, train loss: 0.24889046217915511 (MSE)\n",
      "Iteration 888, train loss: 0.25222628591150004 (MSE)\n",
      "Iteration 900, train loss: 0.25550242992654665 (MSE)\n",
      "Iteration 912, train loss: 0.2587093097077039 (MSE)\n",
      "Iteration 924, train loss: 0.26188199578702503 (MSE)\n",
      "Iteration 936, train loss: 0.2650238658869038 (MSE)\n",
      "Iteration 948, train loss: 0.26790938016287363 (MSE)\n",
      "Iteration 960, train loss: 0.2708372189533485 (MSE)\n",
      "Iteration 972, train loss: 0.2738013634601945 (MSE)\n",
      "Iteration 984, train loss: 0.2765986102542046 (MSE)\n",
      "Iteration 996, train loss: 0.27954216951202265 (MSE)\n",
      "Iteration 1008, train loss: 0.2823190494164373 (MSE)\n",
      "Iteration 1020, train loss: 0.2851830870732293 (MSE)\n",
      "Iteration 1032, train loss: 0.28780953612141125 (MSE)\n",
      "Iteration 1044, train loss: 0.29053683473372965 (MSE)\n",
      "Iteration 1056, train loss: 0.2930862410898401 (MSE)\n",
      "Iteration 1068, train loss: 0.2957490752010426 (MSE)\n",
      "Iteration 1080, train loss: 0.2982893671024376 (MSE)\n",
      "Iteration 1092, train loss: 0.30070249437997143 (MSE)\n",
      "Iteration 1104, train loss: 0.30302004727822157 (MSE)\n",
      "Iteration 1116, train loss: 0.30535289422791034 (MSE)\n",
      "Iteration 1128, train loss: 0.30742208063864757 (MSE)\n",
      "Iteration 1140, train loss: 0.30959171060968466 (MSE)\n",
      "Iteration 1152, train loss: 0.3117032722640476 (MSE)\n",
      "Iteration 1164, train loss: 0.3138094821888774 (MSE)\n",
      "Iteration 1176, train loss: 0.31551885456993056 (MSE)\n",
      "Iteration 1188, train loss: 0.3164304268005015 (MSE)\n",
      "Warning: maximum number of iterations reached. Limit was 1200\n"
     ]
    }
   ],
   "source": [
    "data_frame_extra = create_non_linear_predictors_targets_table()\n",
    "\n",
    "# Separate predictors and target\n",
    "X = data_frame_extra.loc[:, data_frame_extra.columns != 'target'].to_numpy()\n",
    "y = data_frame_extra[\"target\"].to_numpy()\n",
    "\n",
    "model = IncrementalForwardStagewiseRegression(silent=False, max_iter=1_200)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, ошибка сначала падает, но затем начинает расти. Падение происходит в силу малых аппроксимаций большим количеством равномерных распределений [-0.5, 0.5] (не забываем про нормировку), близких к 0 значений. Но когда начинают оставаться большие значений, регрессия не может апроксимировать такие сложные функции как `log` линейными, из-за чего начнинает просто добавлять шум."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
